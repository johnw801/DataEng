# DataEng ‚Äì Echtzeit Data Engineering Pipeline

Dieses Projekt demonstriert eine vollst√§ndige **Streaming Data Pipeline** mit **Kafka**, **Apache Spark Structured Streaming** und **Cassandra**. Ozeanographische Sensordaten werden simuliert, √ºber Kafka gestreamt, von Spark verarbeitet und in Cassandra gespeichert.

---

## Architektur√ºberblick

**Komponenten:**

* **Sensor Simulator:** generiert kontinuierlich Zufallsdaten (Temperatur & Salzgehalt)
* **Kafka (KRaft-Modus):** empf√§ngt & verteilt Streaming-Daten
* **Spark:** liest Kafka-Streams, aggregiert Messwerte und erkennt Anomalien
* **Cassandra:** speichert aggregierte Daten und Anomalien
* **Docker Compose:** orchestriert alle Services in einer isolierten Netzwerkumgebung

---

## ‚öôÔ∏è Voraussetzungen

Vor dem Start sollten folgende Tools installiert sein:

| Komponente            | Empfehlung                  | Hinweis                                                          |
| --------------------- | --------------------------- | ---------------------------------------------------------------- |
| **Docker Desktop**    | ‚â• 4.x                       | Empfohlen f√ºr lokale Entwicklung und Containerverwaltung         |            
| **Java JDK 11**       | Zwingend erforderlich       | Spark ben√∂tigt Java 11 zur Laufzeit                              |
| **Python 3.9+**       | Optional                    | Nur n√∂tig, wenn du den Code au√üerhalb von Docker testen m√∂chtest |

---

## üèÅ Setup & Ausf√ºhrung

### 1Ô∏è. Projekt starten

```bash
docker compose up --build -d
```

Alle Container werden gebaut und gestartet. Healthchecks sorgen daf√ºr, dass Services in der richtigen Reihenfolge initialisiert werden.

---

### 2. Cassandra initialisieren

Bei der **erstmaligen Initialisierung**:

```bash
docker exec -it cassandra cqlsh -u cassandra -p cassandra -f /init.cql
```

Alternativ k√∂nnen auch die SQL-Dateien manuell ausgef√ºhrt werden:

```bash
docker exec -it cassandra cqlsh -u cassandra -p cassandra -f /init.sql
```

Optional kann der Cassandra Standard-Superuser entfernt werden:

```bash
docker exec -it cassandra cqlsh -u cassandra -p cassandra -f /deletesuperuser.cql
```

---

### 3. Services √ºberwachen
F√ºr die √úberwachung der Services wird Docker Desktop empfohlen.

* **Alternativ:**
* **Logs anzeigen:**

  ```bash
  docker logs -f sensor-simulator
  docker logs -f spark-submit-job
  ```
* **Cassandra-Client √∂ffnen:**

  ```bash
  docker exec -it cassandra cqlsh
  ```

---

## Netzwerksicherheit

* Standardm√§√üig sind **alle externen Ports deaktiviert** (au√üer Cassandra-Port `9042` f√ºr PyCharm).
* Ports k√∂nnen bei Bedarf in `docker-compose.yml` aktiviert werden.
* Kafka und Spark Web-UIs sind **optional** (siehe unten).

---

## Optionale Komponenten

### Kafka Web UI aktivieren

```yaml
# In docker-compose.yml entkommentieren:
# kafka-ui:
#   image: provectuslabs/kafka-ui:latest
#   ports:
#     - 8085:8080
```

Zugriff anschlie√üend √ºber [http://localhost:8085](http://localhost:8085)

### Spark UI aktivieren

```yaml
# spark-master:
#   ports:
#     - 7070:7070
# spark-worker:
#   ports:
#     - 8081:8081
```

---

## Zugangsdaten & Sicherheit

* Standardpasswort f√ºr Cassandra:

  ```
  initialespasswortbitteaendern
  ```

  ‚Üí **Bitte unbedingt √§ndern!**

* Kafka Log-Retention:

  ```
  24 Stunden (Datenschutz-konform)
  ```

* `.env` enth√§lt sensible Variablen (`CASSANDRA_USER`, `CASSANDRA_PASSWORD`)

---

## Skalierbarkeit

Mehr Rechenknoten oder Broker k√∂nnen einfach hinzugef√ºgt werden:

```yaml
# Beispiel: zus√§tzlicher Spark Worker
spark-worker-2:
  image: bde2020/spark-worker:3.3.0-hadoop3.3
  environment:
    - SPARK_MASTER=spark://spark-master:7077
  networks:
    - data-pipeline
```

Ebenso k√∂nnen zus√§tzliche Kafka-Broker definiert werden.

---

## Datenfluss

```text
[Sensoren ‚Üí Kafka ‚Üí Spark Streaming ‚Üí Cassandra]
```

* Sensor simuliert Messwerte (JSON)
* Kafka verteilt Daten an Spark
* Spark aggregiert & erkennt Anomalien
* Cassandra speichert Ergebnisse dauerhaft

---

## Hinweise

* **Docker Desktop wird dringend empfohlen**, da es alle ben√∂tigten Dienste lokal b√ºndelt.
* **Java JDK 11 ist zwingend erforderlich**, da Spark darauf basiert.
  √úberpr√ºfe die Installation mit:

  ```bash
  java -version
  # Ausgabe sollte √§hnlich lauten: openjdk version "11.0.x"
  ```


## Autor

Erstellt im Rahmen des Moduls **Data Engineering**
von J.W.
