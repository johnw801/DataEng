services:

  kafka:
    image: confluentinc/cp-kafka:8.1.0
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_LISTENERS: PLAINTEXT://:9092,CONTROLLER://:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_LOG_RETENTION_HOURS: 24 # Datenschutz, Löschung Persistenter Daten nach 24h
    volumes:
      - kafka_data:/var/lib/kafka/data
    healthcheck:
      test: ["CMD", "bash", "-c", "kafka-topics --bootstrap-server localhost:9092 --list || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 6
    networks:
      - data-pipeline

  sensor-simulator:
    build: ./sensor_data
    image: sensor-simulator:latest
    depends_on:
      kafka:
        condition: service_healthy
      spark-master:
          condition: service_healthy
      cassandra:
          condition: service_healthy
    volumes:
      - ./sensor_data:/app
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
    command: >
      bash -c "sleep 60 && python /app/sensordata.py"
    restart: unless-stopped
    networks:
      - data-pipeline

  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    volumes:
      - ./spark:/opt/spark-apps
      - spark_data:/opt/spark-data
      - spark_checkpoints:/tmp/spark-checkpoint
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 15s
      timeout: 10s
      retries: 5
    networks:
      - data-pipeline

  spark-worker:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    ports:
      - "8081:8081" # Für Web-UI
    depends_on:
      spark-master:
        condition: service_healthy
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    volumes:
      - spark_checkpoints:/tmp/spark-checkpoint
    networks:
      - data-pipeline

  spark-submit-job:
    image: bde2020/spark-submit:3.3.0-hadoop3.3
    depends_on:
      kafka:
        condition: service_healthy
      spark-master:
        condition: service_healthy
      cassandra:
        condition: service_healthy
    volumes:
      - ./spark:/opt/spark-apps
      - spark_checkpoints:/tmp/spark-checkpoint
    command:
      [
        "/spark/bin/spark-submit",
        "--master", "spark://spark-master:7077",
        "--packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,com.datastax.spark:spark-cassandra-connector_2.12:3.3.0",
        "/opt/spark-apps/spark_kafka_consumer.py"
      ]
    env_file:
      - .env
    restart: on-failure
    networks:
      - data-pipeline

  cassandra:
      container_name: cassandra
      image: cassandra:4.1
      ports:
        - "9042:9042"   # Für Anbindung an PyCharm
      volumes:
        - ./cassandra/cassandra.yaml:/etc/cassandra/cassandra.yaml
        - cassandra_data:/var/lib/cassandra
        - ./cassandra/init.cql:/init.cql:ro
      healthcheck:
        test: ["CMD-SHELL", "cqlsh -u cassandra -p cassandra -e 'describe keyspaces' || exit 1"]
        interval: 20s
        timeout: 10s
        retries: 10
      networks:
        - data-pipeline

volumes:
  kafka_data:
  spark_data:
  cassandra_data:
  spark_checkpoints:

networks:
  data-pipeline:
    driver: bridge
