services:

  # SENSOR-SIMULATOR – simuliert Echtzeitdaten für Kafka
  sensor-simulator:
    container_name: sensor-simulator
    build: ./sensor_data
    image: sensor-simulator:latest
    depends_on:
      kafka:
        condition: service_healthy
      spark-master:
        condition: service_healthy
      cassandra:
        condition: service_healthy
      spark-submit-job:
        condition: service_started
    volumes:
      - ./sensor_data:/app
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
    restart: unless-stopped
    # Kurze Startverzögerung (2min), um sicherzustellen, dass Cassandra und Submit initialisiert ist
    command: bash -c "sleep 120 && python /app/sensordata.py"
    networks:
      - data-pipeline

  # KAFKA – Messaging-Broker für Streaming-Daten
  kafka:
    container_name: kafka
    image: confluentinc/cp-kafka:8.1.0
    #ports:
     # - 9092:9092 # Optional freigeben für Kafka-Web UI. Service "kafka-ui" muss auch aktiviert werden
    environment:
      # Node- & Listener-Konfiguration; KRaft Modus ohne Zookeeper
      KAFKA_NODE_ID: 1                      # eindeutig pro Broker. Wichtig beim skalieren
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_LISTENERS: PLAINTEXT://:9092,CONTROLLER://:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093

      # Replikation & Aufbewahrung
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 # speichert Offsets nur einmal
      KAFKA_NUM_PARTITIONS: 3    # 3 Sensoren = 3 Partionierungen
      KAFKA_LOG_RETENTION_HOURS: 24  # Datenschutz: alte Daten nach 24h löschen
    volumes:
      - kafka_data:/var/lib/kafka/data
    healthcheck:
      test: [ "CMD-SHELL", "kafka-topics --bootstrap-server localhost:9092 --list || exit 1" ]
      interval: 20s
      timeout: 10s
      start_period: 60s
      retries: 6
    networks:
      - data-pipeline

  # CASSANDRA – NoSQL-Datenbank
  cassandra:
    container_name: cassandra
    image: cassandra:4.1
    ports:
      - "9042:9042"   # Zugriff von extern (PyCharm)
    volumes:
      - ./cassandra/cassandra.yaml:/etc/cassandra/cassandra.yaml
      - cassandra_data:/var/lib/cassandra
      - ./cassandra/init.cql:/init.cql:ro
    healthcheck:
      test: ["CMD-SHELL", "cqlsh -u cassandra -p cassandra -e 'describe keyspaces' || exit 1"]
      interval: 20s
      timeout: 10s
      start_period: 60s
      retries: 10
    networks:
      - data-pipeline

  # SPARK MASTER – Zentrale Steuerung für Spark-Cluster
  spark-master:
    container_name: spark-master
    image: bde2020/spark-master:3.3.0-hadoop3.3
    #ports:
    # - "7070:8080"  # Optional freigeben für Spark Master Web-UI
    volumes:
      - ./spark:/opt/spark-apps
      - spark_data:/opt/spark-data
      - spark_checkpoints:/tmp/spark-checkpoint
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/ || exit 1"]
      interval: 20s
      timeout: 10s
      start_period: 60s
      retries: 5
    networks:
      - data-pipeline

  # SPARK WORKER – Rechenknoten für den Spark-Master
  spark-worker:
    container_name: spark-worker-1
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    #ports:
    # - "7071:8081"  # Optional freigeben für Spark Web-UI
    depends_on:
      spark-master:
        condition: service_healthy
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    volumes:
      - spark_checkpoints:/tmp/spark-checkpoint
    networks:
      - data-pipeline

  # SPARK SUBMIT JOB – führt Spark-Kafka-Cassandra-Pipeline aus
  spark-submit-job:
    container_name: spark-submit-job
    image: bde2020/spark-submit:3.3.0-hadoop3.3
    depends_on:
      kafka:
        condition: service_healthy
      spark-master:
        condition: service_healthy
      cassandra:
        condition: service_healthy
    volumes:
      - ./spark:/opt/spark-apps
      - spark_checkpoints:/tmp/spark-checkpoint
    command:
      [
        "/spark/bin/spark-submit",
        "--master", "spark://spark-master:7077",
        "--packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,com.datastax.spark:spark-cassandra-connector_2.12:3.3.0",
        "/opt/spark-apps/spark_kafka_consumer.py"
      ]
    env_file:
      - .env
    restart: on-failure
    networks:
      - data-pipeline

  #Optional Service einbinden für Kafka UI
  #kafka-ui:
      #image: provectuslabs/kafka-ui:latest
      #ports:
       # - 8085:8080
      #depends_on:
       # kafka:
        #  condition: service_healthy
      #environment:
       # - KAFKA_CLUSTERS_0_NAME=local
        #- KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:9092
        #- KAFKA_CLUSTERS_0_ZOOKEEPER=disabled
      #networks:
       # - data-pipeline

# VOLUMES – persistente Speicherbereiche
volumes:
  kafka_data:
  spark_data:
  cassandra_data:
  spark_checkpoints:

# NETWORKS – interne Docker-Netzwerke
networks:
  data-pipeline:
    driver: bridge # isoliertes Bridge-Netzwerk
